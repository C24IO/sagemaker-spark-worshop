{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"kernel_python_credentials\" : {\"url\": \"http://172.31.35.237:8998/\"}, \"session_configs\": \n",
      "{\"executorMemory\": \"2g\",\"executorCores\": 2,\"numExecutors\":4}}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "echo '{\"kernel_python_credentials\" : {\"url\": \"http://172.31.35.237:8998/\"}, \"session_configs\": \n",
    "{\"executorMemory\": \"2g\",\"executorCores\": 2,\"numExecutors\":4}}' > ~/.sparkmagic/config.json\n",
    "less ~/.sparkmagic/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CHANGE ME ##\n",
    "file_path = \"s3://neilawspublic/ml/data/data.csv\"\n",
    "spark_model_location='s3://<bucket>/models/car_price_prediction_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>22</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "data = spark.read.csv(path=file_path, header=True, quote='\"', sep=\",\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Mileage: integer (nullable = true)\n",
      " |-- Make: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Trim: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Cylinder: integer (nullable = true)\n",
      " |-- Liter: double (nullable = true)\n",
      " |-- Doors: integer (nullable = true)\n",
      " |-- Cruise: integer (nullable = true)\n",
      " |-- Sound: integer (nullable = true)\n",
      " |-- Leather: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "def get_indexer_input(data):\n",
    "    str_cols_value = {}\n",
    "    for c, t in data[data.columns].dtypes:\n",
    "        if t == 'string':\n",
    "            str_cols_value[c] = StringIndexer(inputCol=c, outputCol='indexed_' + c).fit(data)\n",
    "    return str_cols_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Trim': StringIndexer_40caa3ec7e2c4b54610e, 'Make': StringIndexer_46ce96f656ccd971cbb6, 'Type': StringIndexer_4818acc21a4351897db0, 'Model': StringIndexer_48b098f594ec78f0da60}"
     ]
    }
   ],
   "source": [
    "data_test, data_train = data.randomSplit(weights=[0.3, 0.7], seed=10)\n",
    "\n",
    "get_indexer_input = get_indexer_input(data)\n",
    "print (get_indexer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_training(data_train, indexer_input):\n",
    "    x_cols = list(set(data_train.columns) - set(indexer_input.keys() + [\"Price\"]))\n",
    "    str_ind_cols = ['indexed_' + column for column in indexer_input.keys()]\n",
    "    indexers = indexer_input.values()\n",
    "    pipeline_tr = Pipeline(stages=indexers)\n",
    "    data_tr = pipeline_tr.fit(data_train).transform(data_train)\n",
    "    assembler = VectorAssembler(inputCols=x_cols, outputCol=\"features\")\n",
    "    gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Price\", stepSize=0.008, maxDepth=5, subsamplingRate=0.75,\n",
    "                       seed=10, maxIter=20, minInstancesPerNode=5, checkpointInterval=100, maxBins=64)\n",
    "    pipeline_training = Pipeline(stages=[assembler, gbt])\n",
    "    model = pipeline_training.fit(data_tr)\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_testing(model, data_test, indexer_input):\n",
    "    indexers = indexer_input.values()\n",
    "    pipeline_te = Pipeline(stages=indexers)\n",
    "    data_te = pipeline_te.fit(data_test).transform(data_test)\n",
    "    predictions = model.transform(data_te)\n",
    "    predictions.select(\"Price\",\"prediction\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_training(data_train, get_indexer_input)\n",
    "model.save(spark_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-------+-----+---------+-------+-----+---------------+-----+------------+-----+------------+------------+------------+-------------+\n",
      "|Cruise|Cylinder|Doors|Leather|Liter|Make     |Mileage|Model|Price          |Sound|Trim        |Type |indexed_Trim|indexed_Make|indexed_Type|indexed_Model|\n",
      "+------+--------+-----+-------+-----+---------+-------+-----+---------------+-----+------------+-----+------------+------------+------------+-------------+\n",
      "|0     |4       |4    |1      |1.6  |Chevrolet|26191  |AVEO |9041.9062544231|0    |SVM Sedan 4D|Sedan|0.0         |0.0         |0.0         |0.0          |\n",
      "+------+--------+-----+-------+-----+---------+-------+-----+---------------+-----+------------+-----+------------+------------+------------+-------------+\n",
      "\n",
      "+------------------+\n",
      "|prediction        |\n",
      "+------------------+\n",
      "|10236.175823272792|\n",
      "+------------------+"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml import Pipeline\n",
    "import json\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "def get_indexer_input(data):\n",
    "    str_cols_value = {}\n",
    "    for c, t in data[data.columns].dtypes:\n",
    "        if t == 'string':\n",
    "            str_cols_value[c] = StringIndexer(inputCol=c, outputCol='indexed_' + c).fit(data)\n",
    "    return str_cols_value\n",
    "\n",
    "def model_testing(model, data_test, indexer_input):\n",
    "    indexers = indexer_input.values()\n",
    "    pipeline_te = Pipeline(stages=indexers)\n",
    "    data_te = pipeline_te.fit(data_test).transform(data_test)\n",
    "    data_te.show(1,False)\n",
    "    predictions = model.transform(data_test)\n",
    "    predictions.select(\"prediction\").show(10,False)\n",
    "\n",
    "sameModel = PipelineModel.load(path=spark_model_location)\n",
    "\n",
    "j={\"Price\":9041.9062544231,\"Mileage\":26191,\"Make\":\"Chevrolet\",\"Model\":\"AVEO\",\"Trim\":\"SVM Sedan 4D\",\"Type\":\"Sedan\",\"Cylinder\":4,\"Liter\"\n",
    ":1.6,\"Doors\":4,\"Cruise\":0,\"Sound\":0,\"Leather\":1}\n",
    "\n",
    "a=[json.dumps(j)]\n",
    "jsonRDD = sc.parallelize(a)\n",
    "df = spark.read.json(jsonRDD)\n",
    "\n",
    "get_indexer_input = get_indexer_input(df)\n",
    "model_testing(sameModel, df, get_indexer_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
